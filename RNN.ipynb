{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model used is a Recurrent Neural Network (RNN) using Pytorch. An RNN was chosen as it has a couple advantages when searching through a sequence for binding sites or other information that a sequence can tell us. One of the design features allow RNN's to handle temporal dependancies, the effects of previous inputs on the current decision making, which is vital when looking at sequences as the surrounding base pairs change the information that is given from the one being observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making data usable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = \"labeled_df.pkl\"\n",
    "test_df = load_data(pkl_file)\n",
    "Seqs_X = test_df['One_Hot_Sequence'].values\n",
    "Labels_y = test_df['Label'].values\n",
    "Seqs_X_array = np.array([np.array(seq) for seq in Seqs_X])\n",
    "Seqs_X_array = Seqs_X_array.astype(np.float32)\n",
    "Seqs_X_tensor = torch.tensor(Seqs_X_array)\n",
    "Labels_y_array = np.array([int(''.join(map(str, label)), 2) for label in Labels_y])\n",
    "Labels_y_array = Labels_y_array.astype(np.int64)\n",
    "Labels_y_tensor = torch.tensor(Labels_y_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Labels_y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_model(model, criterion, optimizer, input_data, labels, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "Seqs_X_data =  Seqs_X_tensor  \n",
    "Labels_y_data = Labels_y_tensor \n",
    "\n",
    "input_size = 4  \n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = len(np.unique(Labels_y))  \n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1 \n",
    "train_model(model, criterion, optimizer, Seqs_X_tensor, Labels_y_tensor, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and issues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we encountered several challenges that made getting results for the recurrent neural network impossible. The first hurdle we encountered was getting the data from data preperation to work with the model architecture. It took turning the data from a pandas.core.series.Series, to a numpy.array and then finally a torch.Tensor. Once those steps were understood, it was time to move on to the model itself.\n",
    "\n",
    "The model itself was built around the pytorch implementation of a Recurrent Neural Network and gave its own set of challenges that could not be overcome. We tried to experiment with many different parameters to try and get it to work but technical issues arose across multiple computers that were causing them to be frozen and unusable, causing the program to close. After getting the code back working, it would not say what or why it was freezing and leaving us lost what to do next.\n",
    "\n",
    "These challenges highlight the complexities involved in developing and training neural network models, particularly for tasks involving large and complex datasets. While advancements in networks like this offer promising opportunities for addressing complex problems in bioinformatics, the practical implementation of these methods are difficult to say the least. Future research endeavors in this field would benefit from collaboration between biochemistry experts, machine learning practitioners, and computer hardware specialists to mitigate technical challenges and optimize model performance.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
